#!/usr/bin/env python
"""
Script para medir throughput (requests/segundo) del sistema de cachÃ©.

Simula carga concurrente usando threading para medir cuÃ¡ntos requests
por segundo puede manejar el sistema con y sin cachÃ©.

Uso:
    poetry run python scripts/benchmark_throughput.py
"""

import time
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests

# ==================== CONFIGURACIÃ“N ====================

API_BASE_URL = "http://localhost:8000/api/v1"
TOTAL_REQUESTS = 1000
CONCURRENT_WORKERS = 10
TIMEOUT = 5

# ==================== FUNCIONES ====================


def make_request(url: str, bypass_cache: bool = False) -> tuple[bool, float]:
    """
    Realiza un request HTTP GET.

    Args:
        url: URL del endpoint
        bypass_cache: Si True, envÃ­a header de bypass

    Returns:
        Tuple (success: bool, latency: float)
    """
    headers = {"X-Cache-Bypass": "true"} if bypass_cache else {}

    try:
        start = time.time()
        response = requests.get(url, headers=headers, timeout=TIMEOUT)
        latency = time.time() - start

        return (response.status_code == 200, latency)
    except Exception:
        return (False, 0.0)


def measure_throughput(url: str, bypass_cache: bool = False) -> dict:
    """
    Mide throughput con carga concurrente.

    Args:
        url: URL del endpoint
        bypass_cache: Si True, hace bypass del cachÃ©

    Returns:
        Dict con resultados (requests/seg, total_time, etc.)
    """
    print(f"  Ejecutando {TOTAL_REQUESTS} requests con {CONCURRENT_WORKERS} workers...", end=" ", flush=True)

    successful_requests = 0
    failed_requests = 0
    total_latency = 0.0

    start_time = time.time()

    # Ejecutar requests concurrentemente
    with ThreadPoolExecutor(max_workers=CONCURRENT_WORKERS) as executor:
        futures = [executor.submit(make_request, url, bypass_cache) for _ in range(TOTAL_REQUESTS)]

        for future in as_completed(futures):
            success, latency = future.result()
            if success:
                successful_requests += 1
                total_latency += latency
            else:
                failed_requests += 1

    total_time = time.time() - start_time

    # Calcular mÃ©tricas
    requests_per_second = successful_requests / total_time if total_time > 0 else 0
    avg_latency = (total_latency / successful_requests * 1000) if successful_requests > 0 else 0

    print(f"âœ“ ({requests_per_second:.2f} req/s)")

    return {
        "total_requests": TOTAL_REQUESTS,
        "successful": successful_requests,
        "failed": failed_requests,
        "total_time": total_time,
        "requests_per_second": requests_per_second,
        "avg_latency_ms": avg_latency,
    }


# ==================== MAIN ====================


def main():
    """Ejecuta benchmark de throughput."""
    print("ğŸš€ THROUGHPUT BENCHMARK")
    print(f"   API: {API_BASE_URL}")
    print(f"   Total requests: {TOTAL_REQUESTS}")
    print(f"   Concurrent workers: {CONCURRENT_WORKERS}")

    # Verificar servidor
    try:
        response = requests.get(f"{API_BASE_URL.replace('/api/v1', '')}/health", timeout=5)
        if response.status_code != 200:
            print("âŒ Error: Servidor no estÃ¡ saludable")
            return
    except requests.exceptions.RequestException:
        print("âŒ Error: No se puede conectar al servidor")
        return

    url = f"{API_BASE_URL}/stats"

    print("\n" + "=" * 60)
    print("THROUGHPUT: GET /stats")
    print("=" * 60)

    # Warmup
    print("\nğŸ”¥ Warming up cache...")
    requests.get(url, timeout=5)

    # 1. Con cachÃ©
    print("\n1ï¸âƒ£  Throughput CON cachÃ©:")
    with_cache = measure_throughput(url, bypass_cache=False)

    # 2. Sin cachÃ©
    print("\n2ï¸âƒ£  Throughput SIN cachÃ© (baseline):")
    without_cache = measure_throughput(url, bypass_cache=True)

    # Resumen
    print("\n" + "=" * 60)
    print("ğŸ“Š RESULTADOS DE THROUGHPUT")
    print("=" * 60)

    print(f"\nâœ… CON cachÃ©:")
    print(f"   Requests/segundo: {with_cache['requests_per_second']:.2f} req/s")
    print(f"   Tiempo total:     {with_cache['total_time']:.2f}s")
    print(f"   Latencia promedio: {with_cache['avg_latency_ms']:.2f}ms")
    print(f"   Exitosos/Total:   {with_cache['successful']}/{with_cache['total_requests']}")

    print(f"\nâŒ SIN cachÃ©:")
    print(f"   Requests/segundo: {without_cache['requests_per_second']:.2f} req/s")
    print(f"   Tiempo total:     {without_cache['total_time']:.2f}s")
    print(f"   Latencia promedio: {without_cache['avg_latency_ms']:.2f}ms")
    print(f"   Exitosos/Total:   {without_cache['successful']}/{without_cache['total_requests']}")

    # Mejora
    improvement = (
        with_cache["requests_per_second"] / without_cache["requests_per_second"]
        if without_cache["requests_per_second"] > 0
        else 0
    )

    print(f"\nğŸš€ MEJORA:")
    print(f"   Throughput:  {improvement:.2f}x mÃ¡s rÃ¡pido con cachÃ©")

    print("\n" + "=" * 60)
    print("âœ… Benchmark completado")
    print("=" * 60)


if __name__ == "__main__":
    main()
