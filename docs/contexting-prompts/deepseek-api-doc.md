# üß† DeepSeek API ‚Äî Documentaci√≥n para Res√∫menes de Texto

## üìç Endpoint Principal
**POST**  
`https://api.deepseek.com/chat/completions`

---

## üßæ Descripci√≥n
La **DeepSeek Chat API** es compatible con el formato OpenAI y permite generar res√∫menes de texto mediante el modelo `deepseek-chat`. 

Caracter√≠sticas destacadas:
- API s√≠ncrona (respuesta inmediata, sin polling)
- Compatible con SDK de OpenAI
- Context caching autom√°tico (reduce costos hasta 80%)
- JSON output nativo
- Sin l√≠mites artificiales de uso

---

## üöÄ Ejemplo b√°sico (Python con OpenAI SDK)

```python
from openai import OpenAI

# Configurar cliente (compatible con DeepSeek)
client = OpenAI(
    api_key="<your_deepseek_api_key>",
    base_url="https://api.deepseek.com"
)

# Generar resumen
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {
            "role": "system",
            "content": "Eres un asistente que genera res√∫menes concisos en espa√±ol."
        },
        {
            "role": "user",
            "content": "Resume el siguiente texto: [tu texto aqu√≠]"
        }
    ],
    max_tokens=1000,
    temperature=0.7,
    stream=False
)

print(response.choices[0].message.content)
```

---

## ‚öôÔ∏è Par√°metros principales

| Par√°metro | Tipo | Obligatorio | Descripci√≥n |
|-----------|------|-------------|-------------|
| `model` | String | ‚úÖ S√≠ | Modelo a usar: `deepseek-chat` o `deepseek-reasoner` |
| `messages` | Array | ‚úÖ S√≠ | Lista de mensajes del chat (system, user, assistant) |
| `max_tokens` | Integer | ‚ùå No | M√°ximo de tokens a generar (default: 4K, max: 8K) |
| `temperature` | Number | ‚ùå No | Temperatura de muestreo (0-2, default: 1) |
| `top_p` | Number | ‚ùå No | Nucleus sampling (0-1, default: 1) |
| `stream` | Boolean | ‚ùå No | Streaming de respuesta (SSE) |
| `response_format` | Object | ‚ùå No | Forzar JSON output: `{"type": "json_object"}` |

---

## üì¶ Ejemplo de respuesta

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1730000000,
  "model": "deepseek-chat",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Este es el resumen generado..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 450,
    "completion_tokens": 120,
    "total_tokens": 570,
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 450
  }
}
```

---

## üí∞ Pricing (Modelo deepseek-chat)

| Concepto | Precio por 1M tokens |
|----------|---------------------|
| **Input (cache miss)** | $0.28 |
| **Input (cache hit)** | $0.028 (90% descuento) |
| **Output** | $0.42 |

**Ejemplo de costo mensual (300 videos):**
- Transcripci√≥n promedio: 4,700 tokens
- Resumen promedio: 400 tokens
- **Sin cache:** ~$0.45/mes
- **Con cache (80% hit rate):** ~$0.16/mes

---

## üîÅ Context Caching

DeepSeek implementa **context caching autom√°tico en disco**:
- Cache hit si el prefijo del prompt coincide con una solicitud previa
- El contenido cacheado NO consume cuota de input normal
- √ötil para prompts con prefijos repetidos (system messages, few-shot examples)
- TTL: D√≠as/semanas (se limpia autom√°ticamente)

**Campos en response:**
- `prompt_cache_hit_tokens`: Tokens le√≠dos desde cache ($0.028/1M)
- `prompt_cache_miss_tokens`: Tokens procesados normalmente ($0.28/1M)

**Ejemplo de optimizaci√≥n:**

```python
# Primera llamada: construye cache
messages = [
    {"role": "system", "content": "Eres un experto..."},  # Se cachea
    {"role": "user", "content": "Resume: [texto 1]"}
]

# Segunda llamada: reutiliza cache
messages = [
    {"role": "system", "content": "Eres un experto..."},  # Cache hit!
    {"role": "user", "content": "Resume: [texto 2]"}
]
```

---

## üìã JSON Output

Para obtener respuestas estructuradas:

```python
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {
            "role": "system",
            "content": """Genera res√∫menes en formato JSON con esta estructura:
            {
              "summary": "texto del resumen",
              "keywords": ["palabra1", "palabra2"],
              "language": "Spanish"
            }"""
        },
        {"role": "user", "content": "Resume: [tu texto]"}
    ],
    response_format={"type": "json_object"}
)

import json
result = json.loads(response.choices[0].message.content)
```

---

## üîê Autenticaci√≥n

**API Key:**
- Obtener desde: https://platform.deepseek.com/
- Configurar como variable de entorno: `DEEPSEEK_API_KEY`
- **NO usar `x-api-key` header**, usar configuraci√≥n del cliente OpenAI

```python
# Correcto ‚úÖ
client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)

# Incorrecto ‚ùå (no uses headers custom)
# headers = {"x-api-key": "..."}
```

---

## ‚ö†Ô∏è C√≥digos de error HTTP

| C√≥digo | Descripci√≥n | Soluci√≥n |
|--------|-------------|----------|
| **400** | Formato de request inv√°lido | Verificar JSON y par√°metros |
| **401** | API key inv√°lida o faltante | Revisar autenticaci√≥n |
| **429** | Rate limit alcanzado | Reducir frecuencia de requests |
| **500** | Error interno del servidor | Reintentar tras breve espera |
| **503** | Servidor sobrecargado | Reintentar con backoff exponencial |

---

## üéØ Mejores pr√°cticas para res√∫menes

### 1. System Prompt efectivo

```python
system_prompt = """
Eres un asistente especializado en crear res√∫menes concisos de transcripciones de v√≠deos t√©cnicos.

REGLAS:
1. Resumen en espa√±ol (idioma original)
2. M√°ximo 250 palabras
3. Formato: p√°rrafo √∫nico, tono profesional
4. Incluir: tema principal, 3-5 puntos clave
5. Omitir: saludos, despedidas, muletillas
"""
```

### 2. Estructura del user prompt

```python
user_prompt = f"""
Transcripci√≥n del v√≠deo "{video_title}" (duraci√≥n: {duration}):

---
{transcription_text}
---

Genera un resumen siguiendo las reglas del system prompt.
"""
```

### 3. Par√°metros recomendados

```python
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[...],
    max_tokens=500,        # Suficiente para resumen de 250 palabras
    temperature=0.3,       # Baja para respuestas consistentes
    top_p=0.9,            # Nucleus sampling
    response_format={"type": "json_object"}  # Si necesitas estructura
)
```

---

## üß© Integraci√≥n con proyecto

**Dependencias necesarias:**
```bash
poetry add openai  # SDK compatible con DeepSeek
```

**Variables de entorno (.env):**
```bash
DEEPSEEK_API_KEY=sk-...
DEEPSEEK_BASE_URL=https://api.deepseek.com
```

**Configuraci√≥n (src/core/config.py):**
```python
class Settings(BaseSettings):
    DEEPSEEK_API_KEY: str = Field(min_length=10)
    DEEPSEEK_BASE_URL: str = "https://api.deepseek.com"
```

---

## üìä Limitaciones y restricciones

| Aspecto | L√≠mite |
|---------|--------|
| **Context length** | 128K tokens (~100K palabras) |
| **Max output** | 8K tokens (default: 4K) |
| **Rate limit** | Sin l√≠mite documentado (fair use) |
| **Request timeout** | Configurar en cliente (recomendado: 60s) |

---

## üîó Referencias √∫tiles

- **Documentaci√≥n oficial:** https://api-docs.deepseek.com/
- **Platform (API keys):** https://platform.deepseek.com/
- **Pricing details:** https://platform.deepseek.com/pricing
- **API Status:** https://status.deepseek.com/

---

**Nota:** Este documento est√° optimizado para el proyecto youtube-AIsummary. Para casos de uso m√°s complejos (function calling, reasoning mode), consultar documentaci√≥n oficial.
