# üó∫Ô∏è ROADMAP DETALLADO - IA MONITOR
**Versi√≥n:** 1.0  
**Actualizado:** Octubre 2025  
**Metodolog√≠a:** Incremental con validaci√≥n paso a paso

---

## üìñ √çNDICE
- Visi√≥n General
- Fase 0: Planning & Setup
- Fase 1: Infraestructura Base
- Fase 2: Pipeline Core
- Fase 3: API REST
- Fase 4: Workers Async
- Fase 5: Observabilidad
- Fase 6: Testing & CI
- Fase 7: Deployment
- Timeline Semanal

---

## üéØ VISI√ìN GENERAL

### Objetivo del Proyecto
Crear un agregador inteligente que autom√°ticamente:
- Recopila contenidos sobre IA en desarrollo software (YouTube, RSS, podcasts)
- Transcribe audios usando Whisper (local, gratuito)
- Resume textos usando ApyHub API (10 llamadas/d√≠a gratis)
- Clasifica y expone res√∫menes v√≠a API REST y Telegram

### Doble Prop√≥sito
- **Utilidad real:** Mantenerse informado sobre novedades en IA  
- **Portfolio profesional:** Demostrar backend Python moderno con IA funcional

### Stack Tecnol√≥gico
- **Backend:** FastAPI (async) + PostgreSQL + Redis  
- **Workers:** Celery para tareas en background  
- **IA:** Whisper (transcripci√≥n local) + ApyHub API (res√∫menes)  
- **DevOps:** Docker, GitHub Actions, Prometheus + Grafana

---

## üìã FASE 0: PLANNING & SETUP (1 d√≠a)

### Paso 1: Documento de Arquitectura (Project Designer)
**¬øQu√© hacer?**
- Crear `docs/architecture.md` usando el prompt Project Designer
- Decisiones t√©cnicas justificadas (Whisper local vs APIs de pago, ApyHub vs LangChain, Celery vs BackgroundTasks)
- Diagrama Mermaid de arquitectura completa (componentes + flujos)
- Estructura de directorios definitiva
- Roadmap de features priorizadas por fases

**¬øPor qu√© primero?**
- Define QU√â construir y C√ìMO antes de escribir una l√≠nea de c√≥digo
- Evita refactors masivos posteriores (ej: cambiar de MongoDB a PostgreSQL en semana 3)
- Justifica decisiones t√©cnicas ante reclutadores (portfolio = arquitectura pensada)
- Detecta dependencias cr√≠ticas temprano (ej: l√≠mite de 10 llamadas/d√≠a en ApyHub)

**Entregable:**
- `docs/architecture.md` completo con diagramas
- Stack t√©cnico con tabla de pros/contras
- Flujos de procesamiento documentados

**Git:**
```bash
git commit -m "docs: add initial architecture document with diagrams"
```
**Nos da paso a:** Crear estructura del repositorio basada en arquitectura definida.

---

### Paso 2: Estructura del Repositorio
**¬øQu√© hacer?**
- Crear carpetas seg√∫n arquitectura: `src/api/`, `src/services/`, `src/models/`, `src/repositories/`, `src/tasks/`, `tests/`
- Inicializar Git con `.gitignore` (Python, Docker, secretos)
- README b√°sico con descripci√≥n, badges placeholders (CI, coverage)
- `.env.example` con template de variables (sin valores reales)
- `docs/ADR/` para Architecture Decision Records

**¬øPor qu√© esta estructura?**
- Separaci√≥n clara: API / Servicios / Datos (Clean Architecture)
- Tests desde d√≠a 1 = cultura de calidad
- `.env.example` documenta qu√© variables necesita el proyecto
- ADRs justifican decisiones importantes (por qu√© Whisper local, no Deepgram API)

**Entregable:**
- Carpetas vac√≠as pero con `__init__.py`
- README con descripci√≥n del proyecto
- `.gitignore` configurado

**Git:**
```bash
git init
git add .
git commit -m "chore: initial project structure with Clean Architecture"
git remote add origin <tu-repo>
git push -u origin main
```
**Nos da paso a:** Configurar gestor de dependencias reproducible.

---

### Paso 3: Poetry Setup + Dependencias Base
**¬øQu√© hacer?**
- Inicializar Poetry con `poetry init`
- Instalar dependencias core: FastAPI, SQLAlchemy, Alembic, PostgreSQL driver, Redis, Celery
- Instalar dependencias IA: Whisper, yt-dlp (descarga YouTube), httpx (cliente HTTP async)
- Dev dependencies: pytest, black, ruff, mypy, pytest-cov

**¬øPor qu√© ahora?**
- Dependencias fijadas en `poetry.lock` = reproducible en cualquier m√°quina
- Instalar todo de golpe evita conflictos de versiones despu√©s
- Poetry gestiona entornos virtuales autom√°ticamente

**Entregable:**
- `pyproject.toml` con todas las dependencias
- `poetry.lock` generado

**Git:**
```bash
git add pyproject.toml poetry.lock
git commit -m "chore: add Poetry dependencies for FastAPI, Whisper, and testing"
```
**Nos da paso a:** Levantar infraestructura local (BD + cache).

---

## üèóÔ∏è FASE 1: INFRAESTRUCTURA BASE (2-3 d√≠as)

### Paso 4: Docker Compose (Postgres + Redis)
**¬øQu√© hacer?**
- Crear `docker-compose.yml` con servicios: Postgres 15, Redis 7
- Configurar vol√∫menes para persistencia de datos
- Puertos expuestos: Postgres 5432, Redis 6379
- Variables de entorno para usuario/password de BD

**¬øPor qu√© Docker?**
- BD lista en 1 comando (`docker-compose up -d`)
- Mismo entorno en dev/staging/prod (no "funciona en mi m√°quina")
- No contamina sistema local (se borra f√°cilmente)

**Validaci√≥n:**
- `docker-compose up -d` arranca sin errores
- Conexi√≥n a Postgres exitosa con `psql`
- Redis responde a `redis-cli ping` con `PONG`

**Git:**
```bash
git commit -m "feat: add Docker Compose with Postgres and Redis"
```
**Nos da paso a:** Configurar variables de entorno de la aplicaci√≥n.

---

### Paso 5: Config + Variables de Entorno
**¬øQu√© hacer?**
- Crear `src/core/config.py` usando Pydantic Settings
- Validar variables obligatorias: `DATABASE_URL`, `REDIS_URL`, `APYHUB_TOKEN`
- `.env.example` con plantilla (este S√ç va a Git)
- `.env` real (este NO va a Git, usuario debe crearlo copiando el example)

**¬øPor qu√© Pydantic Settings?**
- Validaci√≥n autom√°tica al arrancar (si falta variable, app no inicia)
- Tipado fuerte (no strings m√°gicos)
- 12-factor app compliant (configuraci√≥n fuera del c√≥digo)

**Validaci√≥n:**
- App falla si `.env` no existe (comportamiento esperado)
- App arranca correctamente con `.env` v√°lido

**Git:**
```bash
git add src/core/config.py .env.example
git commit -m "feat: add configuration management with Pydantic Settings"
```
**Nos da paso a:** Crear API base funcional.

---

### Paso 6: FastAPI Base + Health Check
**¬øQu√© hacer?**
- Crear `src/api/main.py` con aplicaci√≥n FastAPI m√≠nima
- Endpoint `/health` que devuelve `{"status": "ok"}`
- Endpoint `/` que devuelve mensaje de bienvenida
- Configurar documentaci√≥n autom√°tica en `/api/docs`

**¬øPor qu√© health check?**
- Valida que FastAPI funciona correctamente
- Necesario para monitoreo futuro (Prometheus, Kubernetes probes)
- Primera victoria r√°pida (motivaci√≥n para continuar)

**Validaci√≥n:**
- `uvicorn src.api.main:app` arranca sin errores
- `curl localhost:8000/health` responde 200 OK
- Swagger UI accesible en `localhost:8000/api/docs`

**Test:**
- Crear `tests/test_health.py` con test del endpoint health
- `pytest tests/test_health.py -v` debe pasar

**Git:**
```bash
git commit -m "feat: add FastAPI base with health endpoint"
git commit -m "test: add health endpoint test"
```
**Nos da paso a:** Configurar ORM y sistema de migraciones.

---

### Paso 7: ORM + Migraciones (Alembic)
**¬øQu√© hacer?**
- Configurar SQLAlchemy `engine` y `sessionmaker` en `src/core/database.py`
- Inicializar Alembic con `alembic init migrations`
- Configurar `alembic.ini` para usar `DATABASE_URL` desde `.env`
- Crear primer modelo: `Source` (tabla de fuentes: YouTube channels, RSS feeds)
- Generar migraci√≥n autom√°tica: `alembic revision --autogenerate`
- Aplicar migraci√≥n: `alembic upgrade head`

**¬øPor qu√© migraciones?**
- Schema de BD versionado (como Git pero para la base de datos)
- Rollback posible si algo falla (`alembic downgrade`)
- Colaboraci√≥n sin romper datos de otros desarrolladores
- Historial de cambios en schema documentado

**Validaci√≥n:**
- Tabla `sources` existe en Postgres
- Tabla `alembic_version` registra la migraci√≥n
- `alembic current` muestra versi√≥n actual

**Git:**
```bash
git commit -m "feat: add SQLAlchemy ORM setup"
git commit -m "feat: add Alembic migrations"
git commit -m "feat: add Source model with first migration"
```
**Nos da paso a:** Implementar servicios core del pipeline.

---

## üß© FASE 2: PIPELINE CORE (1 semana)

### Paso 8: Integraci√≥n DeepSeek (Res√∫menes)
**¬øQu√© hacer?**
- Crear `src/services/summarization_service.py`
- Implementar m√©todo `summarize_text()` que llama a DeepSeek API
- Usar SDK de OpenAI (compatible con DeepSeek)
- Implementar sistema de prompts en `src/services/prompts/`
- Manejo de errores: timeout, respuestas inv√°lidas

**¬øPor qu√© primero?**
- Es el componente externo cr√≠tico del sistema
- Si DeepSeek est√° ca√≠do o cambia API, mejor descubrirlo YA
- Lo m√°s r√°pido de validar (no requiere BD ni otros servicios)
- API s√≠ncrona (m√°s simple que ApyHub job-based)

**Validaci√≥n:**
- Test de integraci√≥n que llama a API real con texto de prueba
- Resumen generado correctamente en espa√±ol
- Sistema de prompts funciona correctamente

**Git:**
```bash
git commit -m "feat: add DeepSeek summarization service with OpenAI SDK"
git commit -m "feat: add prompt engineering system"
git commit -m "test: add DeepSeek integration test"
```
**Nos da paso a:** Implementar descarga de audios.

---

### Paso 9: Descarga de Audio (yt-dlp)
**¬øQu√© hacer?**
- Crear `src/services/downloader_service.py`
- Implementar `download_audio()` que descarga audio de YouTube en MP3
- Implementar `get_video_metadata()` para obtener info sin descargar
- Configurar carpeta temporal `/tmp/ia-monitor/downloads`
- Extraer mejor calidad de audio disponible

**¬øPor qu√© despu√©s de ApyHub?**
- No depende de ApyHub (servicios aislados)
- Genera archivos que el siguiente paso (transcripci√≥n) consumir√°

**Validaci√≥n:**
- Descargar video test de 30 segundos funciona
- Archivo MP3 generado existe y pesa >10KB
- Metadata extra√≠da correctamente (t√≠tulo, duraci√≥n, autor)

**Git:**
```bash
git commit -m "feat: add YouTube audio downloader with yt-dlp"
git commit -m "test: add downloader service test with sample video"
```
**Nos da paso a:** Implementar transcripci√≥n local.

---

### Paso 10: Transcripci√≥n (Whisper Local)
**¬øQu√© hacer?**
- Crear `src/services/transcription_service.py`
- Cargar modelo Whisper (usar **base** para balance velocidad/precisi√≥n)
- Implementar `transcribe_audio()` que convierte MP3 a texto
- Soportar idioma espa√±ol por defecto, configurable
- Implementar variante con **timestamps** (para features futuras)

**¬øPor qu√© Whisper local?**
- Gratuito (vs Deepgram $0.006/min que escala r√°pido)
- Privacidad (audio no sale del servidor)
- Portfolio impresionante (demuestra manejo de modelos ML reales)
- Sin l√≠mites (procesar 1000 videos no cuesta nada)

**Validaci√≥n:**
- Primera ejecuci√≥n descarga modelo (2-3 min, normal)
- Transcripci√≥n de audio de 30s tarda <1 minuto
- Texto generado tiene sentido y est√° en espa√±ol

**Git:**
```bash
git commit -m "feat: add Whisper transcription service"
git commit -m "test: add transcription test with sample audio"
```
**Nos da paso a:** Conectar todos los servicios en un pipeline.

---

### Paso 11: Orquestador del Pipeline
**¬øQu√© hacer?**
- Crear `src/services/content_processor.py`
- Implementar `process_youtube_url()` que orquesta: **descarga ‚Üí transcripci√≥n ‚Üí resumen ‚Üí guardar en BD**
- Manejar limpieza de archivos temporales
- Guardar transcripciones y res√∫menes en BD v√≠a repositories
- Polling simple para esperar resultado de ApyHub (mejorar con Celery despu√©s)

**¬øPor qu√© orquestador?**
- Conecta los 3 servicios aislados (downloader, transcriber, summarizer)
- Un solo punto de entrada para el flujo completo
- F√°cil de testear (mock cada servicio individual)
- Centraliza l√≥gica de *error handling*

**Validaci√≥n:**
- Test E2E: URL de YouTube ‚Üí resumen guardado en BD
- Pipeline completo tarda ~5-10 minutos (esperado para Whisper)
- Archivos temporales eliminados correctamente

**Git:**
```bash
git commit -m "feat: add ContentProcessor orchestrator for full pipeline"
git commit -m "test: add end-to-end pipeline integration test"
```
**Nos da paso a:** Exponer funcionalidad v√≠a API REST.

---

## üåê FASE 3: API REST (3-4 d√≠as)

### Paso 12: Modelos de BD Completos
**¬øQu√© hacer?**
- Crear modelo `Transcription` con campos: `id`, `source_id`, `text`, `audio_path`, `duration`, `language`
- Crear modelo `Summary` con campos: `id`, `transcription_id`, `summary_text`, `metadata`, `tags`
- Definir relaciones SQLAlchemy: `Source 1‚ÜíN Transcriptions`, `Transcription 1‚ÜíN Summaries`
- Generar y aplicar migraciones Alembic

**¬øPor qu√© ahora?**
- Necesitamos persistir resultados del pipeline
- Relaciones bien definidas facilitan queries despu√©s
- Migraciones versionan cambios en schema

**Validaci√≥n:**
- Tablas `transcriptions` y `summaries` existen en BD
- Relaciones funcionan (foreign keys correctas)

**Git:**
```bash
git commit -m "feat: add Transcription and Summary models with relationships"
git commit -m "feat: add migration for transcriptions and summaries tables"
```
**Nos da paso a:** Implementar capa de acceso a datos.

---

### Paso 13: Repository Pattern
**¬øQu√© hacer?**
- Crear `src/repositories/base_repository.py` con operaciones CRUD gen√©ricas
- Implementar `SummaryRepository` extendiendo `BaseRepository`
- Implementar `TranscriptionRepository` con m√©todos espec√≠ficos
- M√©todos principales: `create`, `get_by_id`, `list_all`, `delete`
- M√©todos personalizados: `get_recent_summaries`, `get_by_source`

**¬øPor qu√© Repository Pattern?**
- Abstrae acceso a datos (cambiar de Postgres a otro DB = solo cambiar repos)
- Queries complejas centralizadas
- F√°cil de testear con mocks
- Reutilizable (un repo, muchos servicios)

**Validaci√≥n:**
- Tests unitarios de cada m√©todo del repository
- Operaciones CRUD funcionan correctamente

**Git:**
```bash
git commit -m "feat: add Repository pattern with base class"
git commit -m "feat: add SummaryRepository and TranscriptionRepository"
git commit -m "test: add repository unit tests"
```
**Nos da paso a:** Crear endpoints REST.

---

### Paso 14: Endpoints REST
**¬øQu√© hacer?**
- Crear **schemas Pydantic** en `src/api/schemas/` para request/response
- Implementar rutas en `src/api/routes/summaries.py`:
  - `GET /api/v1/summaries` ‚Äî Listar res√∫menes recientes
  - `GET /api/v1/summaries/{id}` ‚Äî Detalle de resumen espec√≠fico
- Implementar rutas en `src/api/routes/sources.py`:
  - `POST /api/v1/sources` ‚Äî Registrar fuente nueva (YouTube channel, RSS)
  - `GET /api/v1/sources` ‚Äî Listar fuentes activas
- Registrar routers en `main.py`

**¬øPor qu√© estos endpoints?**
- Listar res√∫menes = funcionalidad core visible
- Gesti√≥n de fuentes = permite alimentar el sistema
- Base para frontend o Telegram bot despu√©s

**Validaci√≥n:**
- Swagger UI muestra todos los endpoints
- Tests de integraci√≥n para cada endpoint
- Respuestas correctas con datos reales de BD

**Git:**
```bash
git commit -m "feat: add Pydantic schemas for API responses"
git commit -m "feat: add summaries REST endpoints"
git commit -m "feat: add sources REST endpoints"
git commit -m "test: add API integration tests"
```
**Nos da paso a:** Procesar tareas pesadas en background.

---

## ‚ö° FASE 4: RATE LIMITING & COLAS (6 d√≠as)

**Objetivo:** Implementar sistema de colas con l√≠mite diario para respetar 10 llamadas/d√≠a de ApyHub.

**Contexto cr√≠tico:**
- ApyHub plan gratuito = 10 llamadas/d√≠a
- Sin sistema de colas = videos se pierden si hay >10/d√≠a
- Sistema de colas = videos pendientes se procesan al d√≠a siguiente

---

### Paso 15: Rate Limiter con Redis
**¬øQu√© hacer?**
- Crear `src/services/rate_limiter.py`
- Implementar clase `ApyHubRateLimiter` con Redis
- M√©todos:
  - `get_remaining_calls()` ‚Üí Consulta cu√°ntas llamadas quedan hoy
  - `can_call_api()` ‚Üí True/False si se puede llamar
  - `record_call()` ‚Üí Incrementa contador tras llamada exitosa
  - `reset_daily_counter()` ‚Üí Reinicia a 0 (tarea programada)
- Clave Redis: `apyhub:daily_calls:YYYY-MM-DD`
- TTL autom√°tico: 24 horas

**¬øPor qu√© este orden?**
- Componente aislado, se puede testear independientemente
- No requiere cambios en BD
- Funcionalidad cr√≠tica: previene exceder cuota

**Validaci√≥n:**
- Tests unitarios con Redis mock
- Test de integraci√≥n con Redis real
- Contador se incrementa correctamente
- TTL expira a medianoche

**Git:**
```bash
git commit -m "feat: add ApyHub rate limiter with Redis"
git commit -m "test: add rate limiter unit and integration tests"
```
**Nos da paso a:** Extender modelo Video con campos de cola.

---

### Paso 16: Ampliar Modelo Video
**¬øQu√© hacer?**
- A√±adir campos a `src/models/video.py`:
  - `summary_status`: 'pending' | 'processing' | 'completed' | 'failed'
  - `summary_text`: Texto del resumen generado
  - `summary_attempts`: Contador de intentos (m√°x 3)
  - `summary_error`: √öltimo error si fall√≥
  - `summarized_at`: Timestamp de generaci√≥n exitosa
- Crear √≠ndice en `summary_status` (queries r√°pidas)
- Crear √≠ndice parcial en `created_at WHERE summary_status='pending'`
- Generar migraci√≥n Alembic
- Aplicar migraci√≥n: `alembic upgrade head`

**¬øPor qu√© ahora?**
- Necesitamos persistir estado de cola en BD
- Migraciones antes de l√≥gica = evita inconsistencias

**Validaci√≥n:**
- Tabla `videos` tiene nuevos campos
- √çndices creados correctamente
- Valores por defecto funcionan (`summary_status='pending'`)

**Git:**
```bash
git commit -m "feat: add summary queue fields to Video model"
git commit -m "feat: add migration for video summary queue"
```
**Nos da paso a:** Crear tarea Celery para procesamiento diario.

---

### Paso 17: Tarea Celery Diaria
**¬øQu√© hacer?**
- Crear `src/tasks/daily_summarization.py`
- Implementar funci√≥n `process_pending_summaries()`
  - Obtener hasta 10 videos con `summary_status='pending'`
  - Ordenar por `created_at ASC` (FIFO)
  - Para cada video:
    - Verificar `can_call_api()` del rate limiter
    - Si OK: procesar resumen
    - Si l√≠mite alcanzado: STOP (resto ma√±ana)
  - Actualizar estados en BD seg√∫n resultado
- Integrar con `SummarizationService` existente
- Manejar reintentos (m√°ximo 3, despu√©s ‚Üí 'failed')

**¬øPor qu√© esta l√≥gica?**
- Procesa m√°ximo 10/d√≠a autom√°ticamente
- Videos no procesados quedan en cola para ma√±ana
- Estado persistente en BD = resistente a ca√≠das

**Validaci√≥n:**
- Tarea ejecuta correctamente con 5 videos pending
- Si hay 15 videos, solo procesa 10
- Reintentos funcionan (fallos ‚Üí pending, 3 intentos ‚Üí failed)
- Rate limiter registra llamadas correctamente

**Git:**
```bash
git commit -m "feat: add daily summarization Celery task"
git commit -m "test: add daily summarization task tests"
```
**Nos da paso a:** Programar tarea con Celery Beat.

---

### Paso 18: Celery Beat Scheduler
**¬øQu√© hacer?**
- Configurar Celery Beat en `src/core/celery_app.py`
- A√±adir schedule:
  ```python
  'daily-summarization': {
      'task': 'daily_summarization',
      'schedule': crontab(hour=0, minute=30),  # 00:30 UTC
      'options': {'expires': 3600}
  }
  ```
- Crear script de inicio: `scripts/start_beat.sh`
- Documentar en README c√≥mo arrancar Beat

**¬øPor qu√© 00:30 UTC?**
- Despu√©s de medianoche = contador Redis resetado
- Hora tranquila = menos carga en servidor
- Antes del amanecer = res√∫menes listos por la ma√±ana

**Validaci√≥n:**
- Beat scheduler arranca sin errores
- Tarea se programa correctamente
- Ejecutar manualmente funciona
- Logs muestran pr√≥xima ejecuci√≥n programada

**Git:**
```bash
git commit -m "feat: add Celery Beat scheduler for daily tasks"
git commit -m "docs: add Beat startup instructions to README"
```
**Nos da paso a:** A√±adir m√©tricas para observabilidad.

---

### Paso 19: M√©tricas Prometheus
**¬øQu√© hacer?**
- A√±adir m√©tricas en `src/services/summarization_service.py`:
  - `apyhub_calls_total` (Counter): Llamadas por status
  - `apyhub_calls_remaining` (Gauge): Llamadas restantes hoy
  - `videos_pending_summary` (Gauge): Videos en cola
  - `summarization_duration_seconds` (Histogram): Tiempo de resumen
- Instrumentar rate limiter
- Instrumentar tarea diaria
- Actualizar endpoint `/metrics`

**¬øPor qu√© m√©tricas?**
- Monitorear uso de cuota en tiempo real
- Detectar problemas antes de que se acumulen
- Datos para optimizar sistema

**Validaci√≥n:**
- Endpoint `/metrics` expone nuevas m√©tricas
- M√©tricas se actualizan tras procesar video
- Prometheus scrapea correctamente

**Git:**
```bash
git commit -m "feat: add Prometheus metrics for rate limiting"
```
**Nos da paso a:** Dashboard visual en Grafana.

---

### Paso 20: Dashboard Grafana
**¬øQu√© hacer?**
- Crear dashboard "ApyHub Queue Management"
- Paneles:
  1. Llamadas ApyHub (usadas/restantes hoy)
  2. Videos en cola (pending vs completed)
  3. Tasa de √©xito (% completed vs failed)
  4. Tiempo promedio de resumen
  5. Alertas (rate limit, >50 pending)
- Exportar dashboard como JSON
- Guardar en `docs/grafana-dashboards/`

**¬øPor qu√© dashboard dedicado?**
- Visualizaci√≥n r√°pida de salud del sistema
- Detectar cuellos de botella visualmente
- Portfolio impresionante (no solo c√≥digo, tambi√©n ops)

**Validaci√≥n:**
- Dashboard accesible en Grafana
- Paneles muestran datos reales
- Alertas se activan correctamente

**Git:**
```bash
git commit -m "feat: add Grafana dashboard for queue management"
```
**Nos da paso a:** Workers as√≠ncronos.

---

## ‚ö° FASE 5: WORKERS ASYNC (2-3 d√≠as)

### Paso 21: Celery Setup
**¬øQu√© hacer?**
- Configurar Celery en `src/core/celery_app.py` con Redis como broker
- Crear tarea `src/tasks/process_content_task.py` que ejecuta el pipeline completo
- Configurar Celery Beat para tareas programadas (scraping peri√≥dico de fuentes)
- Script de inicio de worker: `celery -A src.core.celery_app worker`

**¬øPor qu√© Celery?**
- Procesamiento async = API responde inmediato, trabajo en background
- Transcripci√≥n de 1 video tarda 5-10 min (no puede bloquear endpoint HTTP)
- Reintentos autom√°ticos si falla
- Escalable (m√∫ltiples workers en paralelo)

**Validaci√≥n:**
- Worker arranca sin errores
- Tarea encolada se ejecuta correctamente
- Logs muestran progreso del procesamiento

**Git:**
```bash
git commit -m "feat: add Celery configuration with Redis broker"
git commit -m "feat: add process_content async task"
```
**Nos da paso a:** Automatizar procesamiento peri√≥dico.

---

### Paso 22: Jobs Programados (Celery Beat)
**¬øQu√© hacer?**
- Configurar Celery Beat scheduler
- Crear tarea `sync_sources_task.py` que:
  - Consulta fuentes activas en BD
  - Busca nuevos contenidos (√∫ltimos videos de canal YouTube, nuevos posts de RSS)
  - Encola procesamiento de contenidos nuevos
- Programar ejecuci√≥n: cada 6 horas

**¬øPor qu√© automatizaci√≥n?**
- Sistema aut√≥nomo = sin intervenci√≥n manual
- Mantiene res√∫menes actualizados autom√°ticamente
- Demuestra orquestaci√≥n de workers (punto fuerte en portfolio)

**Validaci√≥n:**
- Beat scheduler arranca y programa tareas
- Tarea se ejecuta en horario configurado
- Nuevos contenidos detectados y procesados

**Git:**
```bash
git commit -m "feat: add Celery Beat for scheduled tasks"
git commit -m "feat: add sync_sources periodic task"
```
**Nos da paso a:** Implementar monitoreo y observabilidad.

---

## üìä FASE 6: OBSERVABILIDAD (2 d√≠as)

### Paso 23: Logging Estructurado
**¬øQu√© hacer?**
- Configurar **structlog** para logs estructurados
- Logs en formato JSON con campos: timestamp, level, message, context
- Diferentes niveles por entorno: DEBUG en dev, INFO en prod
- Logs de cada servicio identificados claramente
- Rotar logs diariamente, mantener √∫ltimos 7 d√≠as

**¬øPor qu√© logging estructurado?**
- Debuggear producci√≥n sin logs = adivinar
- JSON logs = f√°cil de parsear con herramientas (Loki, ELK)
- Context enriquecido (source_id, duration, etc.) ayuda a troubleshooting

**Validaci√≥n:**
- Logs generados en formato JSON v√°lido
- Diferentes servicios loggean correctamente
- Rotaci√≥n funciona (archivos por d√≠a)

**Git:**
```bash
git commit -m "feat: add structured logging with structlog"
```
**Nos da paso a:** Agregar m√©tricas para monitoreo.

---

### Paso 24: M√©tricas (Prometheus)
**¬øQu√© hacer?**
- Instalar `prometheus-client` para Python
- Exponer endpoint `/metrics` en FastAPI
- M√©tricas custom:
  - `summaries_generated_total` (contador)
  - `transcription_duration_seconds` (histograma)
  - `apyhub_api_calls_total` (contador)
  - `pipeline_errors_total` (contador por tipo)
- Configurar scraping de Prometheus en `docker-compose.yml`

**¬øPor qu√© m√©tricas?**
- Detectar problemas antes que usuarios se quejen
- Graficar rendimiento en Grafana
- SLOs medibles (ej: 95% de transcripciones <10min)

**Validaci√≥n:**
- Endpoint `/metrics` expone m√©tricas en formato Prometheus
- Prometheus scrapea m√©tricas correctamente
- Valores se actualizan en tiempo real

**Git:**
```bash
git commit -m "feat: add Prometheus metrics instrumentation"
git commit -m "feat: add Prometheus to Docker Compose"
```
**Nos da paso a:** Visualizar m√©tricas en dashboard.

---

### Paso 25: Grafana Dashboard
**¬øQu√© hacer?**
- Agregar Grafana a `docker-compose.yml`
- Configurar datasource Prometheus
- Crear dashboard con paneles:
  - Res√∫menes generados (√∫ltimas 24h)
  - Duraci√≥n promedio de transcripci√≥n
  - Rate de errores del pipeline
  - Uso de API ApyHub (llamadas restantes del d√≠a)

**¬øPor qu√© Grafana?**
- Visualizaci√≥n clara de salud del sistema
- Alertas visuales (ej: errores >10% = panel rojo)
- Portfolio impresionante (no solo c√≥digo, tambi√©n ops)

**Validaci√≥n:**
- Dashboard accesible en `localhost:3000`
- Paneles muestran datos reales
- Gr√°ficos se actualizan autom√°ticamente

**Git:**
```bash
git commit -m "feat: add Grafana dashboard for system metrics"
```
**Nos da paso a:** Implementar suite de tests completa.

---

## ‚úÖ FASE 7: TESTING & CI/CD (2 d√≠as)

### Paso 26: Suite de Tests Completa
**¬øQu√© hacer?**
- Tests unitarios en `tests/unit/` para servicios y repositories
- Tests de integraci√≥n en `tests/integration/` para API y BD
- Tests E2E en `tests/e2e/` para pipeline completo
- Configurar fixtures pytest para datos de prueba
- Objetivo: >80% de cobertura en l√≥gica cr√≠tica

**¬øPor qu√© cobertura alta?**
- Sin tests, cada cambio es ruleta rusa
- Tests = confianza para refactorizar
- Coverage >80% = se√±al de calidad profesional

**Validaci√≥n:**
- `pytest tests/ -v` todos los tests pasan
- `pytest --cov=src` muestra cobertura >80%
- Tests r√°pidos (<2 min total)

**Git:**
```bash
git commit -m "test: add comprehensive test suite with >80% coverage"
```
**Nos da paso a:** Automatizar tests con CI.

---

### Paso 27: GitHub Actions (CI/CD)
**¬øQu√© hacer?**
- Crear `.github/workflows/test.yml`:
  - Trigger en `push` y `pull_request`
  - Setup Python 3.11 + Poetry
  - Levantar Postgres y Redis con `services`
  - Ejecutar `pytest` con coverage
  - Fallar si coverage <80%
- Crear `.github/workflows/lint.yml`:
  - Black, ruff, mypy
  - Fallar si hay errores de formato o tipado

**¬øPor qu√© CI automatizado?**
- Previene merges que rompen tests
- Code review autom√°tico de formato
- Badge verde en README = proyecto mantenido

**Validaci√≥n:**
- Push a rama trigger workflows
- Workflows pasan con c√≥digo actual
- Badge en README muestra estado

**Git:**
```bash
git commit -m "ci: add GitHub Actions for tests and linting"
```
**Nos da paso a:** Preparar deployment a producci√≥n.

---

## üöÄ FASE 8: DEPLOYMENT (2-3 d√≠as)

### Paso 28: Dockerfile Optimizado
**¬øQu√© hacer?**
- Crear Dockerfile multi-stage (builder + runtime)
- Stage 1: Instalar dependencias con Poetry
- Stage 2: Copiar solo lo necesario (sin dev dependencies)
- Usuario no-root por seguridad
- Health check integrado en container

**¬øPor qu√© multi-stage?**
- Imagen final m√°s peque√±a (500MB vs 2GB)
- Menos superficie de ataque (sin compiladores)
- Build cache eficiente

**Validaci√≥n:**
- `docker build -t ia-monitor .` exitoso
- `docker run ia-monitor` arranca correctamente
- Imagen <600MB

**Git:**
```bash
git commit -m "feat: add optimized multi-stage Dockerfile"
```
**Nos da paso a:** Orquestar todos los servicios.

---

### Paso 29: Docker Compose Producci√≥n
**¬øQu√© hacer?**
- Crear `docker-compose.prod.yml` con todos los servicios:
  - API (FastAPI)
  - Worker (Celery worker)
  - Beat (Celery scheduler)
  - Postgres (con volumen persistente)
  - Redis
  - Prometheus
  - Grafana
- Configurar restart policies
- L√≠mites de recursos (CPU, RAM)

**¬øPor qu√© separar dev y prod?**
- Dev usa hot-reload, prod no
- Prod tiene health checks y limits
- Prod usa secretos desde variables, no `.env`

**Validaci√≥n:**
- `docker-compose -f docker-compose.prod.yml up -d` levanta todo
- Todos los servicios saludables
- API accesible desde fuera del container

**Git:**
```bash
git commit -m "feat: add production Docker Compose configuration"
```
**Nos da paso a:** Scripts de deployment.

---

### Paso 30: Scripts de Deployment
**¬øQu√© hacer?**
- Crear `scripts/deploy.sh` que:
  - Pull √∫ltimos cambios de Git
  - Build nueva imagen Docker
  - Stop containers antiguos
  - Start nuevos containers
  - Health check post-deployment
  - Rollback autom√°tico si falla
- Crear `scripts/backup_db.sh` para backups autom√°ticos
- Configurar `cron` job para backups diarios

**¬øPor qu√© scripts?**
- Deployment manual = errores humanos
- Scripts = reproducible y documentado
- Rollback autom√°tico = recovery r√°pido

**Validaci√≥n:**
- Script ejecuta sin errores
- Deployment se completa en <5 minutos
- Rollback funciona si se simula error

**Git:**
```bash
git commit -m "feat: add deployment and backup scripts"
```
**Nos da paso a:** Automatizar deployment.

---

### Paso 31: CD Autom√°tico (GitHub Actions)
**¬øQu√© hacer?**
- Crear `.github/workflows/deploy.yml`:
  - Trigger solo en push a `main`
  - Ejecutar despu√©s de tests exitosos
  - SSH a servidor de producci√≥n
  - Ejecutar script de deployment
  - Notificar en Slack/Discord si falla

**¬øPor qu√© CD?**
- Push a main = deployment autom√°tico
- Sin intervenci√≥n manual
- Faster time to production

**Validaci√≥n:**
- Merge a `main` trigger deployment
- Cambios visibles en producci√≥n en <10 minutos
- Notificaci√≥n recibida

**Git:**
```bash
git commit -m "ci: add continuous deployment workflow"
```
**Nos da paso a:** Documentaci√≥n final.

---

### Paso 32: Documentaci√≥n Final
**¬øQu√© hacer?**
- README completo con:
  - Descripci√≥n del proyecto y valor
  - Arquitectura (diagrama)
  - Instalaci√≥n local paso a paso
  - Endpoints API documentados
  - Decisiones t√©cnicas importantes (links a ADRs)
  - Badges (CI status, coverage, license)
- Actualizar `docs/ADR/` con decisiones finales
- Screenshots de Grafana dashboard
- Video demo de 2 minutos (opcional pero impresionante)

**¬øPor qu√© documentaci√≥n curada?**
- Onboarding de recruiters en <5 minutos
- Proyecto sin docs = proyecto amateur
- ADRs demuestran pensamiento arquitect√≥nico

**Git:**
```bash
git commit -m "docs: complete README with architecture and setup guide"
git commit -m "docs: finalize ADRs for key technical decisions"
```

---

## üìÖ TIMELINE SEMANAL

### Semana 1: Fundaci√≥n
- **Lunes:** Architecture doc + Git setup + Poetry
- **Martes:** Docker Compose + Config + FastAPI base
- **Mi√©rcoles:** ORM + Migraciones + Source model
- **Jueves:** ApyHub integration + Tests
- **Viernes:** Downloader service + Whisper setup

### Semana 2: Pipeline & API
- **Lunes:** Transcription service + Pipeline orchestrator
- **Martes:** Modelos BD completos + Repositories
- **Mi√©rcoles:** API REST endpoints + Schemas
- **Jueves:** Tests de integraci√≥n API
- **Viernes:** Celery setup + Worker tasks

### Semana 3: Rate Limiting & Colas
- **Lunes:** Rate Limiter con Redis (Paso 15)
- **Martes:** Ampliar modelo Video + Migraci√≥n (Paso 16)
- **Mi√©rcoles:** Tarea Celery diaria + Celery Beat (Pasos 17-18)
- **Jueves:** M√©tricas Prometheus (Paso 19)
- **Viernes:** Dashboard Grafana + Tests (Paso 20)

### Semana 4: Workers & Observabilidad
- **Lunes:** Celery workers + Jobs programados (Pasos 21-22)
- **Martes:** Logging estructurado (Paso 23)
- **Mi√©rcoles:** M√©tricas Prometheus + Grafana (Pasos 24-25)
- **Jueves:** Suite de tests completa + CI (Pasos 26-27)
- **Viernes:** Dockerfile + Docker Compose prod (Pasos 28-29)

### Semana 5: Deployment & Docs
- **Lunes:** Scripts de deployment + Backups (Paso 30)
- **Martes:** CD autom√°tico + Validaci√≥n (Paso 31)
- **Mi√©rcoles:** Documentaci√≥n final + ADRs (Paso 32)
- **Jueves:** Optimizaciones + Pulido
- **Viernes:** Demo video + Publicaci√≥n

**Total:** ~5 semanas trabajando 3-4h/d√≠a (a√±adida 1 semana para rate limiting)

---

## ‚úÖ REGLAS DE ORO

### 1. Commits Peque√±os y Descriptivos
- ‚ùå `git commit -m "todo listo"`
- ‚úÖ `git commit -m "feat(api): add summaries endpoint with pagination"`

### 2. Tests ANTES de Merge
- Cada feature debe tener tests antes de merge a main
- Coverage nunca debe bajar del 80%

### 3. Documentaci√≥n Sincronizada
- README siempre actualizado con features nuevas
- ADRs para decisiones importantes

### 4. Branch Strategy
```text
main              ‚Üê Solo c√≥digo funcional + tests
  ‚îú‚îÄ feat/apyhub-integration
  ‚îú‚îÄ feat/whisper-transcription
  ‚îî‚îÄ feat/celery-workers
```

### 5. Validaci√≥n Paso a Paso
- No avanzar al siguiente paso sin validar el actual
- "Funciona en mi m√°quina" no es suficiente (Docker soluciona esto)

---

¬°√âxito con el proyecto! üöÄ
