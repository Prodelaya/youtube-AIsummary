"""
Tareas de Celery para scraping automÃ¡tico de nuevos videos de YouTube.

Este mÃ³dulo contiene las tareas programadas que detectan automÃ¡ticamente
nuevos videos de los canales suscritos y los encolan para procesamiento.

Tareas:
    - sync_youtube_sources_task: Escanea todos los canales activos de YouTube,
      detecta nuevos videos y los encola automÃ¡ticamente.

Uso:
    # Ejecutar manualmente
    from src.tasks.scraping import sync_youtube_sources_task
    sync_youtube_sources_task.delay()

    # Celery Beat (automÃ¡tico cada 6 horas)
    Configurado en src/core/celery_app.py
"""

import logging
import uuid
from datetime import datetime

from celery import Task
from sqlalchemy.orm import Session

from src.core.celery_app import celery_app
from src.core.database import SessionLocal
from src.models.source import Source
from src.models.video import Video
from src.repositories.source_repository import SourceRepository
from src.repositories.video_repository import VideoRepository
from src.services.youtube_scraper_service import (
    ChannelUnavailableError,
    InvalidChannelURLError,
    RateLimitError,
    VideoMetadata,
    YouTubeScraperError,
    YouTubeScraperService,
)
from src.tasks.video_processing import process_video_task

logger = logging.getLogger(__name__)


# ==================== TAREA PRINCIPAL ====================


@celery_app.task(
    name="sync_youtube_sources",
    bind=True,
    max_retries=3,
    default_retry_delay=300,  # 5 minutos entre reintentos
    time_limit=1800,  # 30 minutos timeout total
    soft_time_limit=1600,  # 26 minutos warning
)
def sync_youtube_sources_task(self: Task) -> dict:
    """
    Tarea programada que sincroniza nuevos videos de canales de YouTube.

    Flujo:
        1. Obtiene todas las fuentes activas con type='youtube_channel'
        2. Para cada fuente:
           a. Scraping de Ãºltimos 10 videos (metadata solamente)
           b. Verifica si ya existen en BD (por URL)
           c. Si son nuevos, crea registro con status='pending'
           d. Encola procesamiento automÃ¡tico con process_video_task
        3. Retorna estadÃ­sticas: fuentes, videos encontrados, videos nuevos

    Returns:
        dict: EstadÃ­sticas de la ejecuciÃ³n {
            'sources_scanned': int,
            'videos_found': int,
            'videos_new': int,
            'videos_enqueued': int,
            'errors': int
        }

    Raises:
        No lanza excepciones. Todos los errores son capturados y logueados.

    Example:
        >>> from src.tasks.scraping import sync_youtube_sources_task
        >>> result = sync_youtube_sources_task.delay()
        >>> result.get()  # Espera el resultado
        {'sources_scanned': 3, 'videos_found': 30, 'videos_new': 5, ...}
    """
    logger.info("ğŸ”„ Iniciando sync_youtube_sources_task")

    db: Session = SessionLocal()
    try:
        # Inicializar servicios
        scraper = YouTubeScraperService()
        source_repo = SourceRepository(db)
        video_repo = VideoRepository(db)

        # Obtener fuentes activas de YouTube
        sources: list[Source] = source_repo.get_by_type(
            source_type="youtube_channel", active_only=True
        )

        if not sources:
            logger.warning("âš ï¸ No hay fuentes de YouTube activas")
            return {
                "sources_scanned": 0,
                "videos_found": 0,
                "videos_new": 0,
                "videos_enqueued": 0,
                "errors": 0,
            }

        logger.info(f"ğŸ“º {len(sources)} fuentes de YouTube activas encontradas")

        # EstadÃ­sticas
        stats = {
            "sources_scanned": 0,
            "videos_found": 0,
            "videos_new": 0,
            "videos_enqueued": 0,
            "errors": 0,
        }

        # Procesar cada fuente
        for source in sources:
            try:
                logger.info(f"ğŸ” Scraping fuente: {source.name} ({source.url})")

                # Scraping de Ãºltimos 10 videos
                videos: list[VideoMetadata] = scraper.get_latest_videos(
                    channel_url=source.url, limit=10
                )

                stats["sources_scanned"] += 1
                stats["videos_found"] += len(videos)

                logger.info(f"ğŸ“¹ {len(videos)} videos encontrados en {source.name}")

                # Procesar cada video
                for video_meta in videos:
                    try:
                        # Verificar si ya existe (deduplicaciÃ³n por URL)
                        existing = video_repo.get_by_url(video_meta.url)

                        if existing:
                            logger.debug(f"â­ï¸ Video ya existe: {video_meta.title[:50]}...")
                            continue

                        # Crear nuevo video
                        new_video = _create_video_from_metadata(video_meta, source.id)
                        db.add(new_video)
                        db.flush()  # Obtener el ID sin commit

                        stats["videos_new"] += 1

                        logger.info(f"âœ… Nuevo video detectado: {new_video.title[:60]}...")

                        # Encolar procesamiento automÃ¡tico
                        process_video_task.delay(str(new_video.id))
                        stats["videos_enqueued"] += 1

                        logger.info(f"ğŸ“¤ Video encolado para procesamiento: {new_video.id}")

                    except Exception as e:
                        logger.error(f"âŒ Error procesando video {video_meta.video_id}: {e}")
                        stats["errors"] += 1
                        continue

                # Commit despuÃ©s de procesar todos los videos de una fuente
                db.commit()
                logger.info(f"ğŸ’¾ Cambios guardados para fuente: {source.name}")

            except InvalidChannelURLError as e:
                logger.error(f"âŒ URL invÃ¡lida para {source.name}: {e}")
                stats["errors"] += 1
                # TODO: Marcar fuente como inactiva?
                continue

            except ChannelUnavailableError as e:
                logger.warning(f"âš ï¸ Canal no disponible {source.name}: {e}")
                stats["errors"] += 1
                # TODO: Marcar fuente como inactiva?
                continue

            except RateLimitError as e:
                logger.error(f"ğŸš« Rate limit alcanzado para {source.name}: {e}")
                stats["errors"] += 1
                # Reintento con backoff exponencial
                raise self.retry(exc=e, countdown=60 * (self.request.retries + 1))

            except YouTubeScraperError as e:
                logger.error(f"âŒ Error scraping {source.name}: {e}")
                stats["errors"] += 1
                continue

            except Exception as e:
                logger.exception(f"âŒ Error inesperado procesando fuente {source.name}: {e}")
                stats["errors"] += 1
                continue

        logger.info(
            f"ğŸ‰ sync_youtube_sources_task completada. "
            f"Fuentes: {stats['sources_scanned']}, "
            f"Videos encontrados: {stats['videos_found']}, "
            f"Videos nuevos: {stats['videos_new']}, "
            f"Videos encolados: {stats['videos_enqueued']}, "
            f"Errores: {stats['errors']}"
        )

        return stats

    except Exception as e:
        logger.exception(f"âŒ Error fatal en sync_youtube_sources_task: {e}")
        db.rollback()
        raise

    finally:
        db.close()


# ==================== HELPERS ====================


def _create_video_from_metadata(meta: VideoMetadata, source_id: uuid.UUID) -> Video:
    """
    Crea una instancia de Video desde VideoMetadata.

    Args:
        meta: Metadata del video desde yt-dlp
        source_id: UUID de la fuente (Source)

    Returns:
        Video: Instancia de Video con status='pending'
    """
    return Video(
        id=uuid.uuid4(),
        source_id=source_id,
        youtube_id=meta.video_id,
        title=meta.title,
        url=meta.url,
        duration_seconds=meta.duration_seconds,
        status="pending",
        published_at=meta.published_at,
        extra_metadata={
            "view_count": meta.view_count,
            "like_count": meta.like_count,
            "channel_name": meta.channel_name,
            "thumbnail_url": meta.thumbnail_url,
            "scraped_at": datetime.utcnow().isoformat(),
        },
    )
